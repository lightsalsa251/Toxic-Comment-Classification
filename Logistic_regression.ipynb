{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> wordnet\n",
      "    Downloading package wordnet to /home/nbuser/nltk_data...\n",
      "      Unzipping corpora/wordnet.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> stopwords\n",
      "    Downloading package stopwords to /home/nbuser/nltk_data...\n",
      "      Unzipping corpora/stopwords.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-03-06 15:53:20--  https://github.com/shbm/kaggle_toxic_comments/blob/master/dataset/train.csv.zip?raw=true\n",
      "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/shbm/kaggle_toxic_comments/raw/master/dataset/train.csv.zip [following]\n",
      "--2018-03-06 15:53:20--  https://github.com/shbm/kaggle_toxic_comments/raw/master/dataset/train.csv.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/shbm/kaggle_toxic_comments/master/dataset/train.csv.zip [following]\n",
      "--2018-03-06 15:53:20--  https://raw.githubusercontent.com/shbm/kaggle_toxic_comments/master/dataset/train.csv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27619914 (26M) [application/zip]\n",
      "Saving to: ‘train.csv.zip?raw=true’\n",
      "\n",
      "train.csv.zip?raw=t 100%[===================>]  26.34M   120MB/s    in 0.2s    \n",
      "\n",
      "2018-03-06 15:53:21 (120 MB/s) - ‘train.csv.zip?raw=true’ saved [27619914/27619914]\n",
      "\n",
      "--2018-03-06 15:53:21--  https://github.com/shbm/kaggle_toxic_comments/blob/master/dataset/test.csv.zip?raw=true\n",
      "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
      "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/shbm/kaggle_toxic_comments/raw/master/dataset/test.csv.zip [following]\n",
      "--2018-03-06 15:53:22--  https://github.com/shbm/kaggle_toxic_comments/raw/master/dataset/test.csv.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/shbm/kaggle_toxic_comments/master/dataset/test.csv.zip [following]\n",
      "--2018-03-06 15:53:22--  https://raw.githubusercontent.com/shbm/kaggle_toxic_comments/master/dataset/test.csv.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24577258 (23M) [application/zip]\n",
      "Saving to: ‘test.csv.zip?raw=true’\n",
      "\n",
      "test.csv.zip?raw=tr 100%[===================>]  23.44M   128MB/s    in 0.2s    \n",
      "\n",
      "2018-03-06 15:53:23 (128 MB/s) - ‘test.csv.zip?raw=true’ saved [24577258/24577258]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/shbm/kaggle_toxic_comments/blob/master/dataset/train.csv.zip?raw=true\n",
    "!wget https://github.com/shbm/kaggle_toxic_comments/blob/master/dataset/test.csv.zip?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  test.csv.zip?raw=true\n",
      "  inflating: test.csv                \n",
      "Archive:  train.csv.zip?raw=true\n",
      "  inflating: train.csv               \n"
     ]
    }
   ],
   "source": [
    "!unzip test.csv.zip?raw=true\n",
    "!unzip train.csv.zip?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('test.csv')\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_clean(text):\n",
    "    text = str(text)\n",
    "    text = text.split(' ')\n",
    "    text = \" \".join([w.lower() for w in text])\n",
    "\n",
    "    return text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc(text):\n",
    "    text = re.sub(r'[^A-Za-z\\s]',r' ',text)\n",
    "    text = re.sub(r'\\n',r' ',text)    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop(text):\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    words = text.split(' ')\n",
    "    text = \" \".join([w for w in words if not w in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312735"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = train['comment_text']\n",
    "testing = test['comment_text']\n",
    "\n",
    "training = list(training)+list(testing)\n",
    "len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower\n"
     ]
    }
   ],
   "source": [
    "training = list(map(lower_clean,training))\n",
    "print('lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punc\n"
     ]
    }
   ],
   "source": [
    "training = list(map(punc,training))\n",
    "print('punc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "for i in range(312735):\n",
    "    training[i] = stop(training[i])\n",
    "    if i%10000==0:\n",
    "        print('stop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation edits made username hardcore metallica fan reverted  vandalisms  closure gas voted new york dolls fac  please remove template talk page since retired             '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT = 'comment_text'\n",
    "train[COMMENT] = training[:159571]\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT] = training[159571:]\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train[COMMENT]\n",
    "test_features = test[COMMENT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer()\n",
    "trn_term_doc = vec.fit_transform(train_features)\n",
    "test_term_doc = vec.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = trn_term_doc\n",
    "test_x = test_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    #r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=8, dual=False)\n",
    "    #x_nb = x.multiply(r)\n",
    "    return m.fit(x, y)#, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "fit severe_toxic\n",
      "fit obscene\n",
      "fit threat\n",
      "fit insult\n",
      "fit identity_hate\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros((len(test), len(label_cols)))\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    print('fit', j)\n",
    "    #m,r = get_mdl(train[j])\n",
    "    m = get_mdl(train[j])\n",
    "    preds[:,i] = m.predict_proba(test_x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id':test['id'],'toxic':preds[:,0],'severe_toxic':preds[:,1],'obscene':preds[:,2],'threat':preds[:,3],'insult':preds[:,4],'identity_hate':preds[:,5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('sub.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.311012</td>\n",
       "      <td>0.946247</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.999893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.002866</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.001656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.021174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.005512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  identity_hate    insult   obscene  severe_toxic  \\\n",
       "0  00001cee341fdb12       0.311012  0.946247  0.999753      0.129643   \n",
       "1  0000247867823ef7       0.001295  0.002866  0.000693      0.000885   \n",
       "2  00013b17ad220c46       0.002261  0.010123  0.006456      0.001910   \n",
       "3  00017563c3f7919a       0.000071  0.001305  0.001054      0.002219   \n",
       "4  00017695ad8997eb       0.001238  0.003779  0.002326      0.001504   \n",
       "\n",
       "     threat     toxic  \n",
       "0  0.033859  0.999893  \n",
       "1  0.000649  0.001656  \n",
       "2  0.000573  0.021174  \n",
       "3  0.000322  0.000755  \n",
       "4  0.000831  0.005512  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sub.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcourses\u001b[0m/   \u001b[01;34mnltk_data\u001b[0m/  sub.csv   test.csv.zip?raw=true  train.csv.zip?raw=true\r\n",
      "\u001b[01;34mexamples\u001b[0m/  README.txt  test.csv  train.csv              Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
